{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmSe79J4fdNt"
      },
      "source": [
        "# CoE 163 2nd Semester AY 2022-2023\n",
        "## Software Exercise 04\n",
        "__IMPORTANT__: This notebook does not work as is because it is your obligation to fill in the missing parts. :) Required tasks include:\n",
        "\n",
        "- Formulating appropriate kernels\n",
        "- Invoking the appropriate kernels\n",
        "- Writing a short journal at the end of this notebook\n",
        "\n",
        "-----\n",
        "\n",
        "This notebook instance contains a template notebook for the software exercise. This exercise aims to get you started with CUDA programming on Python.\n",
        "\n",
        "Although Python is inherently not a language for parallel programming, it can still use interfaces to communicate to libraries that can. For this exercise, __Numba__ will be used as the wrapper for NVIDIA CUDA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1h_GxgXH-PZ"
      },
      "source": [
        "##Acknowledgement\n",
        "This SE has been adapted from one of the exercises offered by CS 239 for 2nd Semester AY 2020-2021."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7M7hlODCGHm"
      },
      "source": [
        "## References\n",
        "Here are some references that can potentially help you finish this SE.\n",
        "\n",
        "- [NVIDIA CUDA C Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)\n",
        "- [CUDA Kernels in Numba](https://numba.pydata.org/numba-doc/latest/cuda/kernels.html)\n",
        "- [CUDA Programming in Numba](https://nyu-cds.github.io/python-numba/05-cuda)\n",
        "- [CUDA Thread Indexing](https://blog.usejournal.com/cuda-thread-indexing-fb9910cba084)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5FaQj3nXQlG"
      },
      "source": [
        "## Setup\n",
        "Before playing with this notebook, you need to first enable the GPU by first looking at the toolbar and going to `Runtime > Change runtime type`. Find the Hardware accelerator subfield and choose \"GPU\". Click `Save` to save the changes. If you have ran some code before this change, rerun them to be able to recognize the existence of the GPU.\n",
        "\n",
        "Numba is one of the several Python wrappers that can interface with CUDA. Import the appropriate API by executing `from numba import cuda`. To check whether the import was successful, the `__version__` property can be called."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5ulOOnqcFxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "136e1d79-4207-4637-e82f-d4e8c44c87e1"
      },
      "source": [
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import numba\n",
        "\n",
        "from numba import cuda\n",
        "\n",
        "print(\"Python version:\", sys.version)\n",
        "print(\"Numba version:\", numba.__version__)\n",
        "print(\"Numpy version:\", np.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version: 3.10.12 (main, Jun  7 2023, 12:45:35) [GCC 9.4.0]\n",
            "Numba version: 0.56.4\n",
            "Numpy version: 1.22.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QBeS_5m3N4JX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hs4kMKJXHIn"
      },
      "source": [
        "## Fetching Available GPUs\n",
        "Call `nvidia-smi` on the command line to fetch the available GPUs. This command will then list hardware statistics of each available GPU in this instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doUCSraIWo9i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "500c33a4-29c2-4e3c-a43c-482728018507"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jun 13 11:48:23 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DzBTvCCa3Vm"
      },
      "source": [
        "Now, it's your time to list the available GPUs for our program. Write an appropriate method for calling the list of available GPUs using Numba. If the method used is correct, the subsequent looping code should be able to call the following properties as the elements of the resulting list is an object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxU9o-XhWOCN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9085a15f-e85b-4572-d460-f0e69cf82d47"
      },
      "source": [
        "# TODO: Replace this empty array with the appropriate Numba CUDA call\n",
        "device_list = cuda.gpus\n",
        "\n",
        "print(\"CUDA-supported GPUs available:\")\n",
        "for each_dev in device_list:\n",
        "    print(f\"{each_dev.id}: {each_dev.name} cc v{each_dev.compute_capability[0]}.{each_dev.compute_capability[1]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA-supported GPUs available:\n",
            "0: b'Tesla T4' cc v7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhTandq7bOjJ"
      },
      "source": [
        "## A GPU Parallel Program (Part I)\n",
        "A GPU parallel program consists of a main program and several \"slave\" programs named _kernels_. Each _kernel_ is a sort of routine that is started by the main program but runs independently. These kernels are usually simple routines that process an element of the input(s).\n",
        "\n",
        "CUDA kernels are defined functions that are marked in some way. CUDA kernels  normally look like this.\n",
        "\n",
        "```python\n",
        "@cuda.jit\n",
        "def kernel_self(a):\n",
        "    a += 1\n",
        "\n",
        "@cuda.jit\n",
        "def kernel_out(out, a, b):\n",
        "    out = a + b\n",
        "```\n",
        "\n",
        "Numba recognizes the `@cuda.jit` decorator to mean that the function below it is a CUDA kernel. The first kernel `kernel_self` shows a kernel that updates one of the input parameters. The second kernel `kernel_out` shows a kernel that dumps the result of one or more of the parameters into another resulting parameter. Note that these kernels _do not_ return an output because the output _should be_ redirected to one of the kernel parameters. This is possible because the underlying API that Numba uses is still the C/C++ one, which honors pass-by-reference invocations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VE3ZD7Lfvpi"
      },
      "source": [
        "## Grids, Threads, and Oh My!\n",
        "Developing a CUDA program involves knowledge on the concept of grids, threads, and blocks. When a kernel is executed, the execution actually involves creation of a certain amount of _threads_ that can be grouped into _blocks_. This collection of blocks is then called a _grid_. Each thread executes a copy of the kernel.\n",
        "\n",
        "A rough definition of the terms is as follows:\n",
        "\n",
        "__grid__ -> a collection of blocks\n",
        "\n",
        "__block__ -> a collection of threads\n",
        "\n",
        "__thread__ -> a single instance that executes _a copy_ of the kernel\n",
        "\n",
        "You can think of this as finding an address of a house. The house is a _thread_, which can be addressed as a lot and a block. The lot number is the specific address of the house within a block, and the block, which is analogous to a thread _block_, is a collection of houses congregating on one large piece of land. These blocks and their accompanying roads can then be collectively fenced off as a gated community, which is analogous to a _grid_.\n",
        "\n",
        "A kernel is actually self-aware of its location within the grid in terms of its grid, block, and thread index. The following properties can be called within a kernel to find its address.\n",
        "\n",
        "```python\n",
        "tx = cuda.threadIdx.x # 0-index thread index\n",
        "bx = cuda.blockIdx.x # 0-index block index\n",
        "bdim = cuda.blockDim.x # Shape of block\n",
        "\n",
        "# Absolute address of thread within a single grid\n",
        "pos = bx * bdim + tx\n",
        "```\n",
        "\n",
        "Note that the properties `threadIdx` and `blockIdx` need to have the property `x` appended to it to return an integer index. This is because CUDA supports _multi-dimensional_ blocks and grids. In a NumPy setting, this can be useful especially if 2D or 3D matrices are involved in calculations. For example, a kernel can accept a 3D matrix, which can be defined with a 3D thread dimension. The thread indices can then be called as `threadIdx.x`, `threadIdx.y`, and `threadIdx.z`.\n",
        "\n",
        "This example below is similar to the addressing shown above except that this is for 2D blocks (and therefore, 2D threads).\n",
        "\n",
        "```python\n",
        "tx, ty = cuda.threadIdx.x, cuda.threadIdx.y # 0-index thread index\n",
        "bx = cuda.blockIdx.x # 0-index block index\n",
        "bdimx, bdimy = cuda.blockDim.x, cuda.blockDim.y # Shape of block\n",
        "\n",
        "# Absolute address of thread within a single grid\n",
        "pos = (bx * bdimx * bdimy) + (ty * bdimx) + tx\n",
        "```\n",
        "\n",
        "Noting the routines above, we can see that we have converted these grid, block, and thread indices into a single index denoting the absolute position of a thread within a grid. Think of it as all of the houses laid out in a single line and then assigned a number. Numba has a convenience method for finding such index.\n",
        "\n",
        "```python\n",
        "# The number in the method signifies the dimension of the\n",
        "# block and grid as declared when the kernel is called.\n",
        "\n",
        "# A tuple is returned if the dimension is greater than 1\n",
        "\n",
        "cuda.grid(1) # 0-index absolute position within grid\n",
        "```\n",
        "\n",
        "There are times that CUDA may return an absolute index that is out of bounds with respect to the input or output parameters. Hence, bounds checking is needed to be able to properly execute such kernels.\n",
        "\n",
        "For this exercise, we will not work with multiple grids - only blocks and threads."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOsuoF5Yt-mk"
      },
      "source": [
        "## A GPU Parallel Program (Part II)\n",
        "Now that we have knowledge of a collection of threads, we can create CUDA kernels that work with a specific subset of the input parameters. For example, the CUDA kernel below increments a 2D matrix by element.\n",
        "\n",
        "```python\n",
        "@cuda.jit\n",
        "def increment(a):\n",
        "    x = cuda.threadIdx.x\n",
        "    y = cuda.threadIdx.y\n",
        "\n",
        "    # The indices can also be called using this method\n",
        "    # x, y = cuda.grid(2)\n",
        "\n",
        "    if x < a.shape[0] and y < a.shape[1]:\n",
        "        a[x, y] += 1\n",
        "```\n",
        "\n",
        "Notice that the routine above includes a line to check whether the indices returned by CUDA are within the bounds of the matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDu7-ZK3bBGW"
      },
      "source": [
        "## Matrix Addition Kernels\n",
        "Now, it's your time to implement the different kernels for your simple program. Populate the three kernels with simple routines that add two matrices by element, by row, and by column. You should have a value for the number of blocks and threads in mind. Be careful of out-of-bound exceptions and make sure to use the appropriate grid, block, and thread size!\n",
        "\n",
        "_HINT:_ It's OK to create these kernels for a single block. Note that we will only be operating on a single grid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYAd2m6eW3T3"
      },
      "source": [
        "# WARN: There may be missing decorators in this code, so check before running it.\n",
        "\n",
        "\"\"\"Performs matrix addition element by element\n",
        "\n",
        "This CUDA kernel adds each matching element in a and b\n",
        "and dumps the result to each matching element in c.\n",
        "\n",
        "The sizes of all three matrices should be the same.\n",
        "\n",
        "Parameters\n",
        "----------\n",
        "res : numpy matrix\n",
        "    Matrix resulting in the addition of a and b\n",
        "a   : numpy matrix\n",
        "    First matrix operand\n",
        "b   : numpy matrix\n",
        "    Second matrix operand\n",
        "\"\"\"\n",
        "\n",
        "@cuda.jit\n",
        "def kernel_1t1e(res, a, b):\n",
        "    # TODO: Populate this function\n",
        "    # HINT: Add elements of a and b element by element\n",
        "    #       and save the result to res.\n",
        "    # HINT: The answer is less than 10 lines.\n",
        "\n",
        "    #Let thread be row, block be col\n",
        "    x = cuda.threadIdx.x    #Row\n",
        "    y = cuda.threadIdx.y    #Col\n",
        "\n",
        "    if x < res.shape[0] and y < res.shape[1]:   #Check array boundaries, row-column or depth-row-column?\n",
        "      res[x][y] = a[x][y] + b[x][y]             #Element-wise addition\n",
        "\n",
        "\"\"\"Performs matrix addition by matrix rows\n",
        "\n",
        "This CUDA kernel loops through each row in a and b\n",
        "and adds each matching e on each row. The resulting\n",
        "row is dumped to the matching row in c.\n",
        "\n",
        "The sizes of all three matrices should be the same.\n",
        "\n",
        "Parameters\n",
        "----------\n",
        "res : numpy matrix\n",
        "    Matrix resulting in the addition of a and b\n",
        "a   : numpy matrix\n",
        "    First matrix operand\n",
        "b   : numpy matrix\n",
        "    Second matrix operand\n",
        "\"\"\"\n",
        "@cuda.jit\n",
        "def kernel_1t1r(res, a, b):\n",
        "    # TODO: Populate this function\n",
        "    # HINT: Iterate through each row of a and b together,\n",
        "    #       then add the elements in the rows and save the\n",
        "    #       result to res.\n",
        "    # HINT: The answer is less than 10 lines.\n",
        "\n",
        "    x = cuda.threadIdx.x    #Row\n",
        "    y = cuda.threadIdx.y    #Col\n",
        "\n",
        "    if x < res.shape[0]:                #Check boundaries\n",
        "      for i in range(res.shape[1]):\n",
        "        res[x][i] = a[x][i] + b[x][i]\n",
        "\n",
        "\"\"\"Performs matrix addition by matrix columns\n",
        "\n",
        "This CUDA kernel loops through each column in a and b\n",
        "and adds each matching on each row. The resulting\n",
        "column is dumped to the matching row in c.\n",
        "\n",
        "The sizes of all three matrices should be the same.\n",
        "\n",
        "Parameters\n",
        "----------\n",
        "res : numpy matrix\n",
        "    Matrix resulting in the addition of a and b\n",
        "a   : numpy matrix\n",
        "    First matrix operand\n",
        "b   : numpy matrix\n",
        "    Second matrix operand\n",
        "\"\"\"\n",
        "@cuda.jit\n",
        "def kernel_1t1c(res, a, b):\n",
        "    # TODO: Populate this function\n",
        "    # HINT: Iterate through each column of a and b together,\n",
        "    #       then add the elements in the columns and save the\n",
        "    #       result to res.\n",
        "    # HINT: The answer is less than 10 lines.\n",
        "\n",
        "    # Number of blocks = number of columnd\n",
        "    # Threads/block = number of rows\n",
        "\n",
        "    x = cuda.threadIdx.x    #Row\n",
        "    y = cuda.threadIdx.y    #Col\n",
        "\n",
        "    if y < res.shape[1]:                #Check boundaries\n",
        "      for i in range(res.shape[0]):\n",
        "        res[i][y] = a[i][y] + b[i][y]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D9Em9opctYU"
      },
      "source": [
        "## Running The Kernels\n",
        "Running a kernel is simple - just execute the function like you would a normal Python function. However, it differs from a normal function because can also take additional parameters influencing the amount of kernels to execute.\n",
        "\n",
        "A kernel can be called like this. Note that a list of parameters enclosed by brackets precede the actual parameters to the kernel.\n",
        "\n",
        "```python\n",
        "kernel[bpg, tpb](out, a, b)\n",
        "```\n",
        "\n",
        "The parameters `bpg` and `tpb` denote the number of _blocks_ (per grid) and _threads_ (per block), respectively. As stated before, each of these numbers can take up either a number or a tuple of up to three numbers. Some examples of this type of invocation for a group of 16 threads are as follows.\n",
        "\n",
        "```python\n",
        "# Single block\n",
        "kernel[1, 16](out, a, b)\n",
        "kernel[1, (4, 4)](out, a, b)\n",
        "kernel[1, (8, 2)](out, a, b)\n",
        "\n",
        "# Multiple blocks\n",
        "kernel[4, 4](out, a, b)\n",
        "kernel[2, 8](out, a, b)\n",
        "```\n",
        "\n",
        "Note that by default, Numba runs kernels synchronously. This means that the program executes the next line only when all of the threads has been executed successfully."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cukmet-TrscR"
      },
      "source": [
        "Now, it's your time to execute the kernels. Replace the appropriate comment stubs with your kernel calls. Be mindful of the number of threads and blocks to use for each call!\n",
        "\n",
        "Once you have written the appropriate calls, you can run the stub code below. Make sure that all of the `Kernel x correct?` print outs are `True` to ensure that your kernels yield the correct answer to the addition.\n",
        "\n",
        "_HINT:_ It's OK to execute the kernels for a single block. Note that we will only be operating on a single grid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OVkqX1GW3r0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8296fd27-77a5-4dad-892a-7ad7f1755633"
      },
      "source": [
        "# Edit this matrix size to experiment with its effect on kernel runtime\n",
        "mat_size = (30, 30)\n",
        "\n",
        "# Generate two random integer matrices and one empty matrix to hold the result\n",
        "a, b = np.random.randint(-100, 100, mat_size), np.random.randint(-100, 100, mat_size)\n",
        "c = np.zeros(mat_size)\n",
        "\n",
        "# Generate reference answer to matrix addition\n",
        "ctrl_ans = a + b\n",
        "\n",
        "s_time = time.time()\n",
        "# TODO: Replace this comment with kernel_1t1e call\n",
        "kernel_1t1e[1, mat_size](c, a, b)\n",
        "e_time = time.time()\n",
        "\n",
        "print(\"Kernel 1t1e correct?\", np.all(ctrl_ans == c))\n",
        "print(f\"Execution time: {e_time - s_time:.4f}s\")\n",
        "\n",
        "s_time = time.time()\n",
        "# TODO: Replace this comment with kernel_1t1r call\n",
        "kernel_1t1r[1, (1, mat_size[1])](c, a, b)\n",
        "e_time = time.time()\n",
        "print(\"Kernel 1t1r correct?\", np.all(ctrl_ans == c))\n",
        "print(f\"Execution time: {e_time - s_time:.4f}s\")\n",
        "\n",
        "s_time = time.time()\n",
        "# TODO: Replace this comment with kernel_1t1c call\n",
        "kernel_1t1c[1, (mat_size[0], 1)](c, a, b)\n",
        "e_time = time.time()\n",
        "print(\"Kernel 1t1c correct?\", np.all(ctrl_ans == c))\n",
        "print(f\"Execution time: {e_time - s_time:.4f}s\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kernel 1t1e correct? True\n",
            "Execution time: 0.0035s\n",
            "Kernel 1t1r correct? True\n",
            "Execution time: 0.0021s\n",
            "Kernel 1t1c correct? True\n",
            "Execution time: 0.0045s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht28nB5Or9et"
      },
      "source": [
        "## Which is Faster?\n",
        "The subroutine above is actually the _main_ routine that manages all of the kernels. This also contains a simple runtime measurement to check which of these programs run the fastest. With the results above, write a short journal below this part that answers the following questions or considerations.\n",
        "\n",
        "- Choice of number of threads and blocks, and reasoning behind such\n",
        "- Rank of the three kernels in terms of speed\n",
        "- Why the kernels ran with their appropriate speeds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TCza1j5sQtz"
      },
      "source": [
        "__Write your journal here~!__\n",
        "\n",
        "1. Choice of number of threads and blocks, and reasoning behind such\n",
        "\n",
        "  -I opted to use a single block for all kernels as it is much easier to understand and program. For element-wise matrix addition, I used 2D threads in the form of (row, column) to represent the elements; and since the size of the square matrix used is 20, the tpb argument is set to (20, 20). For the per-row and per-column addition, I respectively used (1, 20) and (20, 1) threads.\n",
        "\n",
        "2. Rank of the three kernels in terms of speed\n",
        "\n",
        "  -Fastest- 1t1r, mid- 1t1c, slowest- 1t1e; but 1t1c and 1t1r runtimes were close based on the three executions done\n",
        "\n",
        "  -1t1r runtimes- 0.0018s, 0.0026s, 0.0019s\n",
        "\n",
        "  -1t1c runtimes- 0.0019s, 0.0027s, 0.0022s\n",
        "\n",
        "  -1t1e runtimes- 0.0025s, 0.0026s, 0.0023s\n",
        "\n",
        "3. Why the kernels ran with their appropriate speeds\n",
        "\n",
        "  -I think that the reason why the element-wise addition generally ran slower is because it involves more operations. 1t1r ran faster than 1t1c as Google Colab is row-major."
      ]
    }
  ]
}